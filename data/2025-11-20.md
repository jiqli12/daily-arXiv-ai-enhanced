<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Impact of Image Resolution on Age Estimation with DeepFace and InsightFace](https://arxiv.org/abs/2511.14689)
*Shiyar Jamo*

Main category: cs.CV

TL;DR: 该研究评估了图像分辨率对DeepFace和InsightFace年龄估计准确性的影响，发现224x224像素是最佳分辨率，过低或过高分辨率都会降低准确性，InsightFace在所有分辨率下都比DeepFace更快。


<details>
  <summary>Details</summary>
Motivation: 自动年龄估计广泛用于年龄验证，但输入图像的分辨率差异很大，需要研究分辨率对年龄估计准确性的影响。

Method: 使用IMDB-Clean数据集的1000张图像，在七种不同分辨率下处理，共生成7000个测试样本，使用DeepFace和InsightFace进行年龄估计，通过MAE、SD和MedAE评估性能。

Result: 两种框架在224x224像素时性能最佳，DeepFace的MAE为10.83年，InsightFace的MAE为7.46年。低分辨率时MAE显著增加，过高分辨率也会降低准确性。InsightFace在所有分辨率下都比DeepFace更快。

Conclusion: 输入图像分辨率对年龄估计准确性有明显且一致的影响，224x224是最佳分辨率，InsightFace在准确性和速度方面都优于DeepFace。

Abstract: Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.

</details>


### [2] [Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images](https://arxiv.org/abs/2511.14702)
*Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Meryem Jabrane,Vicente Grau,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态框架，将ECG电生理信息与AHA-17图谱解剖先验相结合，用于LGE心脏MRI的瘢痕分割，通过时间感知特征融合机制处理非同步采集数据，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: LGE心脏MRI的瘢痕分割因对比度变化和成像伪影而具有挑战性，ECG信号提供补充的生理信息，传导异常有助于定位瘢痕区域。

Method: 提出多模态框架，集成ECG电生理信息和AHA-17图谱解剖先验，引入时间感知特征融合(TAFF)机制，基于采集时间差动态加权融合特征。

Result: 在临床数据集上评估，相比最先进的仅图像基线(nnU-Net)，瘢痕平均Dice分数从0.6149提升至0.8463，精确度0.9115，灵敏度0.9043。

Conclusion: 整合生理和解剖知识使模型能够"超越图像观察"，为稳健且生理基础的心脏瘢痕分割设定了新方向。

Abstract: Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.

</details>


### [3] [Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising](https://arxiv.org/abs/2511.14719)
*Yifan Wang,Liya Ji,Zhanghan Ke,Harry Yang,Ser-Nam Lim,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出了一种零样本框架，通过扩散视频基础模型增强合成视频的真实感，同时保持原始视频的多层次时空结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的合成视频往往缺乏真实感，需要一种能够在不进行微调的情况下提升合成视频真实感的方法，同时保持与原始视频的结构和语义一致性。

Method: 使用扩散视频基础模型，通过辅助模型提取合成视频的结构感知信息（如深度图、语义图和边缘图）来指导生成/去噪过程，确保增强视频在结构和语义层面与原始视频一致。

Result: 实验表明，该方法在保持最先进真实感质量的同时，在结构一致性方面优于现有基线方法。

Conclusion: 该方法是一种简单、通用且强大的合成视频真实感增强方法，能够有效提升合成视频的真实感并保持结构一致性。

Abstract: We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.

</details>


### [4] [ARC Is a Vision Problem!](https://arxiv.org/abs/2511.14761)
*Keya Hu,Ali Cy,Linlu Qiu,Xiaoman Delores Ding,Runqian Wang,Yeyin Eva Zhu,Jacob Andreas,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出VARC框架，将ARC抽象推理问题重新定义为图像到图像转换任务，使用Vision Transformer架构在ARC数据上从头训练，在ARC-1基准上达到60.4%准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ARC旨在促进抽象推理研究，但现有方法多从语言角度处理，而ARC任务本质上是视觉性的。本文希望从视觉中心视角重新构建ARC问题。

Method: 将ARC任务构建为图像到图像转换问题，在"画布"上表示输入以融入视觉先验，使用标准视觉架构（如ViT）进行图像映射，通过测试时训练实现泛化。

Result: 在ARC-1基准上达到60.4%准确率，大幅优于同样从头训练的现有方法，与领先的LLMs竞争，接近人类平均表现水平。

Conclusion: 视觉中心的方法在抽象推理任务上具有显著优势，VARC框架证明了标准视觉架构在ARC问题上的有效性，为抽象推理研究提供了新视角。

Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.

</details>


### [5] [FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation](https://arxiv.org/abs/2511.14712)
*Yunfeng Wu,Jiayi Song,Zhenxiong Tan,Zihao He,Songhua Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法FreeSwim，通过内滑动窗口注意力和交叉注意力覆盖策略，利用预训练的视频扩散Transformer生成超高分辨率视频，解决了传统注意力机制在超高分辨率视频生成中的计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 现代基于Transformer的视频生成器中的注意力机制具有二次时间和内存复杂度，使得端到端训练超高分辨率视频变得极其昂贵。

Method: 采用内滑动窗口注意力机制，并设计了双路径管道，通过交叉注意力覆盖策略将局部注意力的语义内容与具有完整感受野的分支进行引导，确保整体一致性。同时引入交叉注意力缓存策略提高效率。

Result: 实验表明该方法能够以无训练的方式生成具有精细视觉细节的超高分辨率视频，在VBench上表现优异，甚至优于基于训练的方法，同时具有竞争性或改进的效率。

Conclusion: FreeSwim方法在无需额外训练的情况下，成功实现了超高分辨率视频的高效生成，在保持视觉保真度和细节的同时确保了全局一致性。

Abstract: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim

</details>


### [6] [A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments](https://arxiv.org/abs/2511.14742)
*Stefan Cobeli,Kazi Shahrukh Omar,Rodrigo Valença,Nivan Ferreira,Fabio Miranda*

Main category: cs.CV

TL;DR: 提出了一种基于视图的3D城市数据探索方法，使用向量场编码环境视图，通过神经场构建高效的隐式3D环境表示，支持可见性评估、太阳照射评估等城市分析任务。


<details>
  <summary>Details</summary>
Motivation: 尽管3D城市数据集日益增多，但由于计算瓶颈和数据交互复杂性，提取洞察仍然具有挑战性。复杂的3D城市环境几何结构导致高度遮挡，需要大量手动视角调整，使得大规模探索效率低下。

Method: 引入基于神经场的方法构建3D环境的高效隐式表示，支持快速直接查询（视图评估指标计算）和逆向查询（避免遮挡并搜索匹配期望数据模式的视图）。

Result: 通过定量实验、基于真实世界城市挑战的案例研究以及领域专家反馈验证了方法的有效性，在寻找理想视点、分析建筑立面可见性和评估室外空间视野方面表现出色。

Conclusion: 该方法为3D城市数据探索提供了有效的解决方案，支持关键城市分析任务，代码和数据已公开可用。

Abstract: Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.

</details>


### [7] [Vision Large Language Models Are Good Noise Handlers in Engagement Analysis](https://arxiv.org/abs/2511.14749)
*Alexander Vedernikov,Puneet Kumar,Haoyu Chen,Tapio Seppänen,Xiaobai Li*

Main category: cs.CV

TL;DR: 提出一个利用视觉大语言模型(VLMs)来优化视频中参与度识别标注的框架，通过问卷提取行为线索划分数据可靠性，结合课程学习和软标签优化策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 视频数据集中的参与度识别面临主观标签和噪声的挑战，这些因素限制了模型性能。需要解决标注主观性和噪声问题。

Method: 使用VLMs优化标注并指导训练过程，通过问卷提取行为线索将数据分为高可靠性和低可靠性子集，结合课程学习和软标签优化策略逐步引入模糊样本并调整监督以反映不确定性。

Result: 在EngageNet基准测试中（六种特征设置中的三种）最大提升+1.21%，在DREAMS/PAFE基准测试中F1分数分别提升+0.22/+0.06，超越了现有最先进方法。

Conclusion: 该方法证明了使用VLMs解决标签主观性问题的优势，通过在精炼的高可靠性子集上训练并结合课程学习策略，能够显著提升参与度识别性能。

Abstract: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.

</details>


### [8] [Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers](https://arxiv.org/abs/2511.14751)
*Yutian Chen,Yuheng Qiu,Ruogu Li,Ali Agha,Shayegan Omidshafiei,Jay Patrikar,Sebastian Scherer*

Main category: cs.CV

TL;DR: Co-Me是一种无需重新训练或微调的视觉几何Transformer加速机制，通过轻量级置信度预测器对token进行不确定性排序，选择性合并低置信度token来减少计算量，同时保持空间覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有的基于相似性的合并或剪枝方法在视觉几何Transformer中效果有限，需要一种能够可靠识别Transformer关注区域的方法来加速推理，同时不牺牲性能。

Method: 提出置信度引导的token合并方法，训练轻量级置信度预测器来评估token的不确定性，然后选择性合并低置信度的token，从而减少计算复杂度。

Result: 在VGGT和MapAnything上分别实现了11.3倍和7.2倍的加速效果，使视觉几何Transformer能够应用于实时3D感知和重建任务。

Conclusion: Co-Me提供了一种有效的视觉几何Transformer加速方案，能够显著提升推理速度而不损失性能，适用于多视角和流式视觉几何Transformer。

Abstract: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.

</details>


### [9] [UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning](https://arxiv.org/abs/2511.14760)
*Rui Tian,Mingfei Gao,Haiming Gang,Jiasen Lu,Zhe Gan,Yinfei Yang,Zuxuan Wu,Afshin Dehghan*

Main category: cs.CV

TL;DR: UniGen-1.5是一个统一的多模态大语言模型，在图像理解、生成和编辑方面具有先进能力。通过改进模型架构和训练流程，增强了图像理解和生成能力，并解锁了强大的图像编辑功能。


<details>
  <summary>Details</summary>
Motivation: 基于UniGen模型，全面增强模型架构和训练流程，旨在加强图像理解和生成能力，同时解锁强大的图像编辑能力。

Method: 提出了统一的强化学习策略，通过共享奖励模型联合改进图像生成和编辑；设计了轻量级的编辑指令对齐阶段，显著提高编辑指令理解能力。

Result: 实验结果显示UniGen-1.5在理解和生成方面表现出竞争力，在GenEval和ImgEdit上分别获得0.89和4.31的总体分数，超越了BAGEL等最先进模型，性能与GPT-Image-1等专有模型相当。

Conclusion: UniGen-1.5成功实现了图像理解、生成和编辑的统一建模，通过创新的训练策略在多个基准测试中达到了最先进的性能水平。

Abstract: We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Bias in, Bias out: Annotation Bias in Multilingual Large Language Models](https://arxiv.org/abs/2511.14662)
*Xia Cui,Ziyi Huang,Naeemeh Adel*

Main category: cs.CL

TL;DR: 本文提出了一个理解NLP数据集中标注偏见的综合框架，区分了指令偏见、标注者偏见以及情境和文化偏见，并综述了检测方法和缓解策略，旨在为多语言大语言模型开发更公平和文化基础的标注流程。


<details>
  <summary>Details</summary>
Motivation: NLP数据集中的标注偏见是多语言大语言模型开发的主要挑战，特别是在文化多样性环境中。来自任务框架、标注者主观性和文化不匹配的偏见会扭曲模型输出并加剧社会危害。

Method: 提出了一个综合框架来理解标注偏见，区分不同类型的偏见；综述了检测方法（包括标注者间一致性、模型分歧和元数据分析）；并概述了主动和被动的缓解策略，包括多样化标注者招募、迭代指南改进和事后模型调整。

Result: 贡献包括：(1) 标注偏见的类型学；(2) 检测指标的综合；(3) 适用于多语言环境的基于集成的偏见缓解方法；(4) 标注过程的伦理分析。

Conclusion: 这些见解旨在为LLMs提供更公平和文化基础的标注流程，促进多语言大语言模型在文化多样性环境中的公平发展。

Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.

</details>


### [11] [Streamlining Industrial Contract Management with Retrieval-Augmented LLMs](https://arxiv.org/abs/2511.14671)
*Kristi Topollai,Tolga Dimlioglu,Anna Choromanska,Simon Odie,Reginald Hui*

Main category: cs.CL

TL;DR: 本文提出了一个基于检索增强生成的模块化框架，用于自动化合同管理中的问题修订识别和优化，在真实低资源条件下达到80%以上准确率。


<details>
  <summary>Details</summary>
Motivation: 合同管理涉及条款审查和谈判，但自动化这一流程面临标注数据稀缺和非结构化历史合同丰富的挑战。

Method: 采用检索增强生成（RAG）流水线，集成合成数据生成、语义条款检索、可接受性分类和基于奖励的对齐方法。

Result: 与行业合作伙伴共同开发和评估，系统在识别和优化问题修订方面均达到80%以上准确率。

Conclusion: 该系统在真实低资源条件下表现出色，为加速合同修订工作流程提供了实用解决方案。

Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

</details>


### [12] [Quadratic Term Correction on Heaps' Law](https://arxiv.org/abs/2511.14683)
*Oscar Fontanelli,Wentian Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.

</details>


### [13] [Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries](https://arxiv.org/abs/2511.14685)
*Kiera McCormick,Rafael Martínez-Galarza*

Main category: cs.CL

TL;DR: 研究探讨LLM嵌入是否能编码从科学测量获得的天体物理统计量，分析提示对编码的影响以及语言中哪些方面对编码物理信息最重要。


<details>
  <summary>Details</summary>
Motivation: 利用LLM在跨领域泛化和上下文学习方面的能力，研究其是否能编码通常仅从科学测量获得并通过文本描述松散编码的物理信息。

Method: 使用天体物理学作为测试平台，通过稀疏自编码器从文本中提取可解释特征，分析提示对物理量编码的影响以及语言中最重要的编码方面。

Result: 研究发现提示在LLM编码物理量中发挥作用，并识别出语言中对编码物理信息最重要的方面。

Conclusion: LLM嵌入能够编码物理统计信息，提示策略和语言特定方面对有效编码物理信息至关重要。

Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

</details>


### [14] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: 使用大型语言模型为历史法语和中文文本生成标注数据，通过微调spaCy模型显著提升了词性标注、词形还原和命名实体识别在特定历史时期的性能。


<details>
  <summary>Details</summary>
Motivation: 解决历史文献和低资源自然语言处理面临的标注数据有限和领域不匹配问题，为计算人文研究提供更好的工具支持。

Method: 利用大型语言模型生成历史文本的标注数据，然后用这些数据微调spaCy模型，应用于16-20世纪法语和1900-1950年中文文本。

Result: 在特定历史时期的测试中，词性标注、词形还原和命名实体识别任务都取得了显著提升。

Conclusion: 领域特定模型对历史文本处理至关重要，即使相对有限的合成数据也能显著改善低资源语料库的自然语言处理工具性能。

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


### [15] [Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances](https://arxiv.org/abs/2511.14693)
*Rishu Kumar Singh,Navneet Shreya,Sarmistha Das,Apoorva Singh,Sriparna Saha*

Main category: cs.CL

TL;DR: 本文提出了VALOR框架，用于多模态、多轮客户支持对话中的投诉分析，通过结合文本投诉和视觉证据实现细粒度分类，在复杂投诉场景中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有投诉分析方法主要依赖单模态短文本内容，需要处理多模态、多轮对话中用户同时提供文本投诉和视觉证据的复杂场景。

Method: 提出VALOR框架，采用多专家推理设置，使用大规模生成模型和思维链提示进行决策，通过语义对齐分数和元融合策略确保模态间一致性。

Result: 在标注细粒度方面和严重性标签的多模态投诉数据集上评估，VALOR始终优于基线模型，特别是在信息分布在文本和图像中的复杂投诉场景中。

Conclusion: 该研究强调了多模态交互和专家验证在实际投诉理解系统中的价值，支持联合国可持续发展目标SDG 9和SDG 12。

Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

</details>


### [16] [Subword Tokenization Strategies for Kurdish Word Embeddings](https://arxiv.org/abs/2511.14696)
*Ali Salehi,Cassandra L. Jacobs*

Main category: cs.CL

TL;DR: 本文比较了库尔德语词嵌入的三种分词策略（词级、基于语素、BPE），发现BPE在形态相似性评估中存在偏差，仅覆盖28.6%测试用例，而语素模型覆盖68.7%。综合评估显示基于语素的分词在嵌入空间组织和语义结构方面更优。


<details>
  <summary>Details</summary>
Motivation: 研究库尔德语等低资源语言的词嵌入分词策略，特别关注形态相似性保持任务中的评估偏差问题。

Method: 开发基于BiLSTM-CRF的形态分割器，使用最小手动标注进行引导训练，在Word2Vec嵌入上比较三种分词方法，采用包括相似性保持、聚类质量和语义组织的综合评估指标。

Result: BPE在形态相似性上表现看似更优，但仅评估28.6%测试用例，而语素模型评估68.7%。综合评估显示基于语素的分词在嵌入空间组织、语义邻域结构和跨形态复杂度覆盖方面更优。

Conclusion: 低资源语言处理中覆盖感知评估的重要性，基于语素的分词策略为低资源语言处理提供了更好的选择。

Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

</details>


### [17] [Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance](https://arxiv.org/abs/2511.14709)
*Raha Aghaei,Ali A. Kiaei,Mahnaz Boush,Mahan Rofoosheh,Mohammad Zavvar*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型在变革研发过程中的多种功能，通过自动化知识发现、促进假设生成、整合跨学科见解和促进创新生态系统内的合作，显著提升研究过程的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何通过其多种功能来改进和加速研发流程，解决传统研发过程中效率低下和创新周期长的问题。

Method: 通过广泛分析科学文献、专利数据库和实验数据，利用大型语言模型实现更灵活和知情的研发工作流程。

Result: 大型语言模型显著提高了研发过程的效率和效果，加速了创新周期，并降低了突破性想法进入市场的时间。

Conclusion: 大型语言模型通过其多方面的功能，在变革研发过程中发挥着关键作用，能够显著提升创新效率和加速技术转化。

Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [SkillGen: Learning Domain Skills for In-Context Sequential Decision Making](https://arxiv.org/abs/2511.14670)
*Ruomeng Ding,Wei Cheng,Minglai Shao,Chen Zhao*

Main category: cs.AI

TL;DR: SkillGen是一个基于技能的ICL框架，通过构建动作中心图、识别高效用动作和检索逐步技能，为结构化顺序推理生成细粒度的上下文感知提示，在多个基准测试中显著提升LLM的决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在顺序决策中难以同时满足三个关键原则：关注决策关键信息、提供步骤级粒度、最小化专家标注依赖。LLM的ICL效果对提示质量高度敏感，需要更有效的提示设计方法。

Method: SkillGen框架包含三个核心步骤：1) 从采样轨迹构建动作中心的领域级图；2) 通过时间差分信用分配识别高效用动作；3) 检索逐步技能生成细粒度、上下文感知的提示。

Result: 在ALFWorld、BabyAI和ScienceWorld基准测试中，使用开源和专有LLM，SkillGen实现了持续增益，平均提高进度率5.9%-16.5%。

Conclusion: 关注高效用片段支持任务可识别性，并为更有效的ICL提示设计提供信息。SkillGen通过技能基础的ICL框架显著提升了LLM在顺序决策任务中的表现。

Abstract: Large language models (LLMs) are increasingly applied to sequential decision-making through in-context learning (ICL), yet their effectiveness is highly sensitive to prompt quality. Effective prompts should meet three principles: focus on decision-critical information, provide step-level granularity, and minimize reliance on expert annotations through label efficiency. However, existing ICL methods often fail to satisfy all three criteria simultaneously. Motivated by these challenges, we introduce SkillGen, a skill-based ICL framework for structured sequential reasoning. It constructs an action-centric, domain-level graph from sampled trajectories, identifies high-utility actions via temporal-difference credit assignment, and retrieves step-wise skills to generate fine-grained, context-aware prompts. We further present a theoretical analysis showing that focusing on high-utility segments supports task identifiability and informs more effective ICL prompt design. Experiments on ALFWorld, BabyAI, and ScienceWorld, using both open-source and proprietary LLMs, show that SkillGen achieves consistent gains, improving progress rate by 5.9%-16.5% on average across models.

</details>


### [19] [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)
*Parya Dolatyabi,Mahdi Khodayar*

Main category: cs.AI

TL;DR: 本文应用异构智能体强化学习框架解决大规模停电后配电网恢复问题，通过HAPPO算法实现互联微电网的协调恢复，相比传统方法具有更快收敛速度和更高恢复功率。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法和基于价值的强化学习方法在处理配电网恢复问题时计算效率低且难以扩展，需要解决非线性约束下的顺序开关操作和分布式能源资源协调问题。

Method: 采用异构智能体强化学习框架，通过HAPPO算法训练去中心化的执行器策略，使用集中式评论家计算优势值进行稳定策略更新，在OpenDSS环境中通过可微分惩罚信号强制执行操作限制。

Result: 在IEEE 123总线和IEEE 8500节点系统上的实验表明，HAPPO相比DQN、PPO、MAES、MAGDPG、MADQN、平均场RL和QMIX等方法，实现了更快的收敛速度、更高的恢复功率和更平滑的多种子训练。

Conclusion: 在HARL框架中引入微电网级别的异构性，为复杂的配电网恢复问题提供了一个可扩展、稳定且约束感知的解决方案。

Abstract: Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [A Unified Compositional View of Attack Tree Metrics](https://arxiv.org/abs/2511.14717)
*Benedikt Peterseim,Milan Lopuhaä-Zwakenberg*

Main category: cs.CR

TL;DR: 本文通过基于gs-幺半范畴的组合理论，为攻击树及其度量提供了函子语义学，将攻击树视为字符串图，并将攻击树度量对应为通道范畴的函子。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击树度量处理方法要么未能包含重要度量，要么过于抽象而无法提供定义具体度量的系统方法。需要一种既通用又具体的系统化方法来定义攻击树度量。

Method: 将攻击树视为字符串图，展示攻击树组件形成通道范畴（一种特定的gs-幺半范畴），攻击树度量对应为通道范畴的函子。

Result: 该特征化既足够通用以包含所有常见的攻击树度量，又足够具体以通过逻辑结构定义攻击树度量。

Conclusion: 基于gs-幺半范畴的组合理论为攻击树度量提供了满意的系统化处理方法，解决了现有方法的不足。

Abstract: Attack trees (ATs) are popular graphical models for reasoning about the security of complex systems, allowing for the quantification of risk through so-called AT metrics. A large variety of different such AT metrics have been proposed, and despite their wide-spread practical use, no systematic treatment of attack tree metrics so far is fully satisfactory. Existing approaches either fail to include important metrics, or they are too general to provide a useful systematic way for defining concrete AT metrics, giving only an abstract characterisation of their behaviour. We solve this problem by developing a compositional theory of ATs and their functorial semantics based on gs-monoidal categories. Viewing attack trees as string diagrams, we show that components of ATs form a channel category, a particular type of gs-monoidal category. AT metrics then correspond to functors of channel categories. This characterisation is both general enough to include all common AT metrics, and concrete enough to define AT metrics by their logical structure.

</details>
