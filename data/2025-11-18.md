<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Towards Assume-Guarantee Verification of Abilities in Stochastic Multi-Agent Systems](https://arxiv.org/abs/2511.10649)
*Wojciech Jamroga,Damian Kurpiewski,Łukasz Mikulski*

Main category: cs.MA

TL;DR: 本文提出了几种用于概率交替时序逻辑与不完全信息的假设-保证验证方案，证明了方案的正确性并讨论了完备性，同时提出了一个新的非概率交替逻辑变体。


<details>
  <summary>Details</summary>
Motivation: 战略能力模型检查是一个极其困难的问题，特别是在具有不完全信息的智能体在随机环境中行动的现实情况下。假设-保证推理可以在这里提供很大帮助，将复杂问题分解为一小组更容易的子问题。

Method: 提出了几种用于概率交替时序逻辑与不完全信息的假设-保证验证方案，并证明这些方案的正确性。同时提出了一个新的非概率交替逻辑变体，其中战略模态捕获"至多实现φ"的概念。

Result: 证明了所提出验证方案的正确性，并讨论了它们的完备性。新的逻辑变体提供了类似于Levesque"仅知道"逻辑的战略表达能力。

Conclusion: 假设-保证验证方案为处理具有不完全信息的智能体在随机环境中的战略能力模型检查问题提供了一种有效的分解方法，新的逻辑变体扩展了战略模态的表达能力。

Abstract: Model checking of strategic abilities is a notoriously hard problem, even more so in the realistic case of agents with imperfect information, acting in a stochastic environment. Assume-guarantee reasoning can be of great help here, providing a way to decompose the complex problem into a small set of easier subproblems.
  In this paper, we propose several schemes for assume-guarantee verification of probabilistic alternating-time temporal logic with imperfect information. We prove the soundness of the schemes, and discuss their completeness. On the way, we also propose a new variant of (non-probabilistic) alternating-time logic, where the strategic modalities capture "achieving at most $\varphi$," analogous to Levesque's logic of "only knowing."

</details>


### [2] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: 提出一个理论框架，将合作博弈论归因与过程奖励建模相结合，将系统级评估转化为智能体信用和响应级信号，为LLM多智能体训练提供从全局评估到局部监督的统一路径。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中LLM的训练方法缺乏将系统级评估与智能体级和消息级学习连接起来的原理性方法。

Method: 结合合作博弈论归因（如Shapley值）和过程奖励建模，在成功案例中基于Shapley的信用分配公平分配结果并细化为每消息奖励；在失败案例中通过首次错误定位产生修复感知偏好。

Result: 产生的信号具有局部性、有符号性和信用守恒性，有界、合作且直接兼容基于强化或偏好的后训练。

Conclusion: 这是一个概念性贡献，提出了理论基础和训练信号，为LLM多智能体训练提供了统一且可审计的从全局评估到局部监督的路径。

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [3] [Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging](https://arxiv.org/abs/2511.10712)
*Qinfeng Li,Miao Pan,Jintao Chen,Fu Teng,Zhiqiang Shen,Ge Su,Hao Peng,Xuhong Zhang*

Main category: cs.CR

TL;DR: 本文提出MergeBarrier防御方法，通过破坏线性模式连接性来防止未经授权的模型合并窃取，在保持高安全性的同时实现可忽略的性能损失。


<details>
  <summary>Details</summary>
Motivation: 模型合并技术虽然能高效扩展大语言模型，但也带来了模型合并窃取的新威胁，现有防御机制无法同时满足主动防御、开源兼容性和高性能保持这三个关键保护属性。

Method: 提出MergeBarrier防御方法，通过破坏保护模型与其同源模型之间的线性模式连接性，消除有效模型合并所需的低损失路径。

Result: 大量实验表明，MergeBarrier能有效防止模型合并窃取，同时仅带来可忽略的准确性损失。

Conclusion: MergeBarrier提供了一种即插即用的主动防御方案，成功解决了模型合并窃取问题，同时满足所有关键保护要求。

Abstract: Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.

</details>


### [4] [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714)
*Shuaitong Liu,Renjue Li,Lijia Yu,Lijun Zhang,Zhiming Liu,Gaojie Jin*

Main category: cs.CR

TL;DR: BadThink是一种针对思维链提示的后门攻击，通过诱导模型产生冗余推理过程来增加计算成本，同时保持最终输出的正确性，从而隐蔽地降低推理效率。


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽然提升了大型语言模型的推理能力，但也引入了计算效率这一新的攻击面。作者旨在揭示这种先前未被探索的漏洞，展示针对CoT系统的新型复杂攻击。

Method: 通过基于中毒的微调策略实现攻击，采用基于LLM的迭代优化过程生成高度自然的污染数据，将"过度思考"行为嵌入模型中。

Result: 在多个最先进模型和推理任务上的实验表明，BadThink能持续增加推理轨迹长度，在MATH-500数据集上实现了超过17倍的增长，同时保持隐蔽性和鲁棒性。

Conclusion: 这项工作揭示了推理效率可能被隐蔽操纵的关键漏洞，展示了一类针对CoT系统的新型复杂攻击，强调了在提升模型推理能力时需要考虑的安全风险。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce "overthinking" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.

</details>


### [5] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: PISanitizer是一种针对长上下文LLM的提示注入防御方法，通过识别和清理潜在注入令牌来消除攻击影响。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入防御方法主要针对短上下文设计，在长上下文场景下效果有限，因为注入指令只占长上下文很小部分，防御难度大。

Method: 基于两个观察：(1)提示注入攻击本质是构建强制LLM遵循的指令；(2)LLM使用注意力机制关注关键输入令牌。PISanitizer首先让LLM遵循上下文中的任意指令，然后清理驱动指令遵循行为的高注意力令牌。

Result: 广泛评估表明PISanitizer能成功防止提示注入、保持实用性、优于现有防御方法、效率高，并对基于优化的强自适应攻击具有鲁棒性。

Conclusion: PISanitizer为攻击者带来困境：注入指令越有效强制LLM遵循，就越可能被PISanitizer清理，从而有效防御长上下文LLM的提示注入攻击。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [6] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: AFLGopher是一种可行性感知的定向模糊测试方法，通过改进距离计算机制，显著提高了到达目标代码和触发已知漏洞的效率。


<details>
  <summary>Details</summary>
Motivation: 现有定向模糊测试的引导机制在计算控制流距离时缺乏可行性感知，导致效率低下。

Method: 提出可行性感知的距离计算方法，包括基于有限轨迹预测所有分支可行性的分类方法，以及运行时可行性更新机制。

Result: AFLGopher在到达目标速度上比现有最佳定向模糊测试工具快2.52-3.76倍，在触发已知漏洞上快4.52-5.60倍。

Conclusion: 可行性感知的定向模糊测试能显著提高测试效率，AFLGopher在多个基准测试中表现优于现有方法。

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [7] [Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation](https://arxiv.org/abs/2511.10863)
*Yiping Ma,Yue Guo,Harish Karthikeyan,Antigoni Polychroniadou*

Main category: cs.CR

TL;DR: Armadillo是一个具有抗破坏性的安全聚合系统，能够在联邦学习环境中抵御恶意客户端的攻击，确保聚合结果仅受客户端在预定义合法范围内输入的影响。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习安全聚合方案要么客户端成本高，要么需要多轮通信。Armadillo旨在实现高效、低轮次的安全聚合，同时保持计算轻量级。

Method: 采用两层安全聚合协议和恶意客户端消除协议，结合零知识证明技术，仅需简单算术计算和3轮通信完成聚合。

Result: Armadillo在每轮安全聚合中仅需3轮通信，同时保持服务器和客户端的计算轻量级，相比现有方案更高效。

Conclusion: Armadillo通过创新的两层聚合和恶意客户端消除协议，实现了高效、低轮次的安全聚合，为联邦学习提供了实用的安全解决方案。

Abstract: This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients (within the tolerated threshold) can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, they either incur high per-client cost (Chowdhury et al. CCS '22) or require many rounds (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo completes each secure aggregation in 3 rounds while keeping the server and clients computationally lightweight.

</details>


### [8] [On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing](https://arxiv.org/abs/2511.10933)
*Yunyi Ni,Ziyu Yang,Ze Niu,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 本文研究扩散模型图像编辑对鲁棒水印的威胁，证明扩散变换会使水印信息丢失，并提出针对性攻击算法，能几乎完全去除水印同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散模型的图像生成和编辑技术的出现，传统的鲁棒水印方案面临新的安全威胁，需要研究这些新技术如何影响水印的生存能力。

Method: 通过理论分析和实验验证，研究扩散驱动的图像编辑如何降解水印，并提出一种引导扩散攻击算法，在生成过程中明确针对并擦除水印信号。

Result: 在最新的深度学习水印方案上评估，攻击后水印恢复率接近零，同时再生图像保持高视觉保真度。

Conclusion: 讨论了此类水印去除能力的伦理影响，并为未来水印策略在生成AI时代提供更具弹性的设计指南。

Abstract: Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.

</details>


### [9] [Gynopticon: Consensus-Based Cheating Detection System for Competitive Games](https://arxiv.org/abs/2511.10992)
*Jeuk Kang,Jungheum Park*

Main category: cs.CR

TL;DR: GYNOPTICON是一个基于用户共识的作弊检测框架，通过客户端轻量级检测和服务器端投票系统来识别游戏作弊行为，为竞争性在线游戏提供隐私保护的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决竞争性游戏类型（如MOBA、FPS、RTS）中作弊检测研究不足的问题，同时避免传统内核级反作弊方案带来的隐私和安全风险。

Method: 集成轻量级客户端检测机制和服务器端投票系统：客户端检测可疑活动后向服务器投票，服务器聚合投票建立共识来区分作弊者和合法玩家。

Result: 在受控模拟和真实FPS环境中的评估验证了系统的可行性和有效性，能够可靠检测作弊用户，并证明系统在长期游戏管理中的适用性和可持续性。

Conclusion: GYNOPTICON代表了传统反作弊系统的用户驱动、基于共识的替代方案，为竞争性在线游戏提供了实用且保护隐私的解决方案。

Abstract: Cheating in online games poses significant threats to the gaming industry, yet most prior research has concentrated on Massively Multiplayer Online Role-Playing Games (MMORPGs). Competitive genres-such as Multiplayer Online Battle Arena (MOBA), First Person Shooter (FPS), Real Time Strategy (RTS), and Action games-remain underexplored due to the difficulty of detecting cheating users and the demand for complex data and techniques. To address this gap, many game companies rely on kernel-level anti-cheat solutions, which, while effective, raise serious concerns regarding user privacy and system security. In this paper, we propose GYNOPTICON, a novel cheating detection framework that leverages user consensus to identify abnormal behavior. GYNOPTICON integrates a lightweight client-side detection mechanism with a server-side voting system: when suspicious activity is identified, clients cast votes to the server, which aggregates them to establish consensus and distinguish cheaters from legitimate players. This architecture enables transparency, reduces reliance on intrusive monitoring, and mitigates privacy risks. We evaluate GYNOPTICON in both a controlled simulation and a real-world FPS environment. Simulation results verify its feasibility and requirements, while real-world experiments confirm its effectiveness in reliably detecting cheating users. Furthermore, we demonstrate the system's applicability and sustainability for long-term game management using public datasets. GYNOPTICON represents a user-driven, consensus-based alternative to conventional anti-cheat systems, offering a practical and privacy-preserving solution for competitive online games.

</details>


### [10] [SALT-V: Lightweight Authentication for 5G V2X Broadcasting](https://arxiv.org/abs/2511.11028)
*Liu Cao,Weizheng Wang,Qipeng Xie,Dongyu Wei,Lyutianyang Zhang*

Main category: cs.CR

TL;DR: SALT-V是一个混合认证框架，通过协议分层解决V2X通信中的认证困境，结合ECDSA的安全性和GMAC的效率，实现即时认证和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统公钥方案（如ECDSA）验证延迟高（2ms）不适合防碰撞，对称方法（如TESLA）效率高但密钥披露延迟大（20-100ms），都无法满足5G NR-V2X对即时认证和计算效率的严格要求。

Method: 采用分层协议：10%流量使用ECDSA签名（BOOT帧）建立发送者信任，90%消息使用轻量级GMAC操作（DATA帧），核心创新是临时会话标签（EST）白名单机制和Bloom过滤器集成。

Result: 平均计算时间0.035ms（比纯ECDSA快57倍），端到端延迟1ms，开销41字节，可线性扩展到2000辆车，满足实时V2X部署的所有安全关键要求。

Conclusion: SALT-V是首个实用解决方案，能够满足实时V2X部署的所有安全关键要求，成功解决了V2X认证的基本权衡问题。

Abstract: Vehicle-to-Everything (V2X) communication faces a critical authentication dilemma: traditional public-key schemes like ECDSA provide strong security but impose 2 ms verification delays unsuitable for collision avoidance, while symmetric approaches like TESLA achieve microsecond-level efficiency at the cost of 20-100 ms key disclosure latency. Neither meets 5G New Radio (NR)-V2X's stringent requirements for both immediate authentication and computational efficiency. This paper presents SALT-V, a novel hybrid authentication framework that reconciles this fundamental trade-off through intelligent protocol stratification. SALT-V employs ECDSA signatures for 10% of traffic (BOOT frames) to establish sender trust, then leverages this trust anchor to authenticate 90% of messages (DATA frames) using lightweight GMAC operations. The core innovation - an Ephemeral Session Tag (EST) whitelist mechanism - enables 95% of messages to achieve immediate verification without waiting for key disclosure, while Bloom filter integration provides O(1) revocation checking in 1 us. Comprehensive evaluation demonstrates that SALT-V achieves 0.035 ms average computation time (57x faster than pure ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles, making it the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment.

</details>


### [11] [Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach](https://arxiv.org/abs/2511.11249)
*Melih Coşğun,Mert Gençtürk,Sinem Sav*

Main category: cs.CR

TL;DR: 本文提出了联邦归一化方法，通过安全交换归一化参数来模拟集中式归一化的效果，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据分布在不同客户端，传统本地归一化在非IID数据下效果差，而集中式归一化违背联邦学习的去中心化原则。

Method: 提出联邦归一化方法，通过安全多方计算和同态加密技术，在保护隐私的前提下协作计算归一化参数。

Result: 联邦归一化能达到与集中式归一化相当的性能，同时不破坏数据本地性。

Conclusion: 联邦归一化是联邦学习中有效的预处理方法，能在保护隐私的同时提升模型性能。

Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).

</details>


### [12] [Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts](https://arxiv.org/abs/2511.11250)
*Biagio Boi,Christian Esposito*

Main category: cs.CR

TL;DR: 本研究评估大型语言模型在检测Solana和Algorand智能合约中OWASP漏洞的能力，通过提示工程、微调和混合方法进行实验，发现提示工程具有通用鲁棒性，而微调在语义较少的语言如TEAL上表现更好。


<details>
  <summary>Details</summary>
Motivation: 智能合约在去中心化环境中作为关键组件，如果代码设计不当会带来潜在风险。现有研究主要关注EVM生态系统，缺乏对非EVM平台（如Solana和Algorand）的漏洞检测研究。

Method: 创建了Rust（Solana）和PyTeal（Algorand）的合成标注数据集，基于OWASP漏洞分类法。评估了三种LLM配置：提示工程、微调和混合方法，比较它们在不同漏洞类别上的性能。

Result: 提示工程实现了通用鲁棒性，而微调在语义较少的语言（如TEAL）上提高了精确率和召回率。分析了Solana和Algorand架构差异对漏洞表现和可检测性的影响。

Conclusion: 基于LLM的方法在智能合约静态漏洞检测中是可行的，前提是将领域特定数据和分类整合到训练流程中。

Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.

</details>


### [13] [Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions](https://arxiv.org/abs/2511.11347)
*Shaowei Guan,Hin Chi Kwok,Ngai Fong Law,Gregor Stiglic,Vivian Hui*

Main category: cs.CR

TL;DR: 本文综述了医疗领域检索增强生成(RAG)应用的隐私风险，分析了23篇相关文献，提出了涵盖数据存储、传输、检索和生成阶段的隐私保护框架，并评估了17篇隐私保护策略文献，指出了临床验证不足、标准化评估框架缺失等关键差距。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)在临床和生物医学工作流程中应用日益广泛，但受保护健康信息(PHI)暴露等隐私风险尚未得到一致缓解，需要系统分析隐私挑战并提供保护机制。

Method: 通过管道结构化框架分析23篇医疗RAG应用文献，涵盖数据存储、传输、检索和生成阶段，识别潜在故障模式及其在威胁模型和系统机制中的根本原因，并评估17篇隐私保护策略文献。

Result: 评估揭示了关键差距，包括临床验证不足、标准化评估框架缺失和自动化评估工具缺乏。提出了基于这些限制的可操作方向。

Conclusion: 为研究人员和从业者提供了理解医疗RAG隐私漏洞的结构化框架，并为开发既具有临床有效性又具备强大隐私保护能力的系统提供了路线图。

Abstract: Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.

</details>


### [14] [SEAL: Subspace-Anchored Watermarks for LLM Ownership](https://arxiv.org/abs/2511.11356)
*Yanbo Dai,Zongjie Li,Zhenlan Ji,Shuai Wang*

Main category: cs.CR

TL;DR: SEAL是一个子空间锚定水印框架，通过将多比特签名嵌入到模型的潜在表示空间中来保护LLM的知识产权，支持白盒和黑盒验证，具有高有效性、保真度、效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有IP保护方法存在局限性：模型指纹技术只能识别架构不能确定所有权，传统后门水印方法容易被微调或知识蒸馏等后处理操作移除。需要一种更鲁棒的水印方法来保护LLM的知识产权。

Method: 利用模型编辑技术将选定锚点样本的隐藏表示与预定义的正交比特向量对齐，在保持模型原始事实预测的同时嵌入水印，使水印功能无害且隐蔽。

Result: 在多个基准数据集和六个主要LLM上的实验表明，SEAL相比11种现有方法在有效性、保真度、效率和鲁棒性方面表现优越，即使在对手知道水印机制和签名的情况下仍保持强验证性能。

Conclusion: SEAL框架为LLM提供了有效的知识产权保护方案，通过子空间锚定水印技术实现了鲁棒的所有权验证，解决了现有方法的局限性。

Abstract: Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation.
  We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.

</details>


### [15] [SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses](https://arxiv.org/abs/2511.11381)
*Gioliano de Oliveira Braga,Pedro Henrique dos Santos Rocha,Rafael Pimenta de Mattos Paixão,Giovani Hoff da Costa,Gustavo Cavalcanti Morais,Lourenço Alves Pereira Júnior*

Main category: cs.CR

TL;DR: 本文系统化分析了Wi-Fi CSI作为生物识别技术的安全性，揭示了现有研究在评估方法、威胁模型和安全指标方面的系统性不一致问题，并提出了统一评估框架来暴露隐藏的风险集中问题。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI作为生物识别技术虽然被多次提出并报告了高精度，但缺乏对其安全属性、对抗弹性和方法一致性的系统理解，需要从安全角度进行系统化知识整理。

Method: 通过分析现有工作在感知基础设施、信号表示、特征管道、学习模型和评估方法等方面的差异，构建统一评估框架，使用安全相关指标（如每类EER、FCS和基尼系数）来揭示传统报告方法隐藏的风险集中问题。

Result: 研究发现存在系统性不一致：依赖聚合精度指标、有限的FAR/FRR/EER报告、缺乏每用户风险分析、很少考虑威胁模型或对抗可行性。实证分析揭示了包括重放、几何模仿和环境扰动在内的具体攻击面。

Conclusion: 本文阐明了当前CSI生物识别的安全边界，为严格评估、可重复实验和未来研究方向提供了指导方针，为安全社区提供了对Wi-Fi CSI生物识别技术及其作为认证原语适用性的结构化、证据驱动的重新评估。

Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

</details>


### [16] [Automated Side-Channel Analysis of Cryptographic Protocol Implementations](https://arxiv.org/abs/2511.11385)
*Faezeh Nasrabadi,Robert Künnemann,Hamed Nemati*

Main category: cs.CR

TL;DR: 本文通过二进制分析和逆向工程构建了WhatsApp的首个形式化模型，证明了前向安全性，识别了克隆攻击，并发现了实现与规范之间的功能差距。同时引入了将侧信道泄漏合约集成到协议模型中的方法，揭示了传统规范方法无法发现的严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析大型闭源应用（如WhatsApp）的加密协议实现安全性，特别是针对功能漏洞和微架构侧信道攻击的全面安全分析。

Method: 结合CryptoBap二进制分析和Ghidra逆向工程提取WhatsApp形式化模型，扩展CryptoBap框架集成硬件泄漏合约，并使用DeepSec协议验证器进行分析。

Result: 证明了WhatsApp的前向安全性，识别了已知的克隆攻击，发现了实现与规范的功能差距，并识别了允许侧信道攻击者获取受害者联系人信息的隐私攻击。

Conclusion: 该方法能够揭示传统规范方法无法检测的关键漏洞，为加密协议实现的安全分析提供了新的方法论，特别是在侧信道攻击防护方面具有重要意义。

Abstract: We extract the first formal model of WhatsApp from its implementation by combining binary-level analysis (via CryptoBap) with reverse engineering (via Ghidra) to handle this large closed-source application. Using this model, we prove forward secrecy, identify a known clone-attack against post-compromise security and discover functional gaps between WhatsApp's implementation and its specification. We further introduce a methodology to analyze cryptographic protocol implementations for their resilience to side-channel attacks. This is achieved by extending the CryptoBap framework to integrate hardware leakage contracts into the protocol model, which we then pass to the state-of-the-art protocol prover, DeepSec. This enables a detailed security analysis against both functional bugs and microarchitectural side-channel attacks. Using this methodology, we identify a privacy attack in WhatsApp that allows a side-channel attacker to learn the victim's contacts and confirm a known unlinkability attack on the BAC protocol used in electronic passports.
  Key contributions include (1) the first formal model of WhatsApp, extracted from its binary, (2) a framework to integrate side-channel leakage contracts into protocol models for the first time, and (3) revealing critical vulnerabilities invisible to specification-based methods.

</details>


### [17] [HetDAPAC: Leveraging Attribute Heterogeneity in Distributed Attribute-Based Private Access Control](https://arxiv.org/abs/2511.11549)
*Shreya Meel,Sennur Ulukus*

Main category: cs.CR

TL;DR: 本文提出了一种异构分布式属性私有访问控制(HetDAPAC)框架，通过将非敏感属性验证集中化处理，在保持敏感属性隐私的同时提高了系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统DAPAC系统对所有属性采用相同的隐私保护级别，但并非所有属性都是敏感的，这种统一的隐私约束会降低系统效率。为了利用用户属性的异构隐私需求，需要设计更灵活的访问控制方案。

Method: 提出HetDAPAC框架，将N个属性中的N-D个非敏感属性验证转移到中央服务器处理，仅对D个敏感属性保持DAPAC的分布式验证架构。设计了两种方案：一种提高效率但存在下载不平衡，另一种实现服务器间下载平衡。

Result: 第一个方案将速率从1/(2K)提高到1/(K+1)，第二个方案实现平衡下载且速率为(D+1)/(2KD)。

Conclusion: HetDAPAC框架通过区分敏感和非敏感属性的隐私需求，在保持敏感属性隐私的同时显著提高了系统效率，为属性验证系统提供了更实用的解决方案。

Abstract: Verifying user attributes to provide fine-grained access control to databases is fundamental to attribute-based authentication. Either a single (central) authority verifies all the attributes, or multiple independent authorities verify the attributes distributedly. In the central setup, the authority verifies all user attributes, and the user downloads only the authorized record. While this is communication efficient, it reveals all user attributes to the authority. A distributed setup prevents this privacy breach by letting each authority verify and learn only one attribute. Motivated by this, Jafarpisheh~et~al. introduced an information-theoretic formulation, called distributed attribute-based private access control (DAPAC). With $N$ non-colluding authorities (servers), $N$ attributes and $K$ possible values for each attribute, the DAPAC system lets each server learn only the single attribute value that it verifies, and is oblivious to the remaining $N-1$. The user retrieves its designated record, without learning anything about the remaining database records. The goal is to maximize the rate, i.e., the ratio of desired message size to total download size. However, not all attributes are sensitive, and DAPAC's privacy constraints can be too restrictive, negatively affecting the rate. To leverage the heterogeneous privacy requirements of user attributes, we propose heterogeneous (Het)DAPAC, a framework which off-loads verification of $N-D$ of the $N$ attributes to a central server, and retains DAPAC's architecture for the $D$ sensitive attributes. We first present a HetDAPAC scheme, which improves the rate from $\frac{1}{2K}$ to $\frac{1}{K+1}$, while sacrificing the privacy of a few non-sensitive attributes. Unlike DAPAC, our scheme entails a download imbalance across servers; we propose a second scheme achieving a balanced per-server download and a rate of $\frac{D+1}{2KD}$.

</details>
